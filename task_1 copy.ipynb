{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bd69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_as: train_classifier.py\n",
    "import os, json, random, shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "ROOT = Path(\"dataset\")          # root dataset folder produced earlier\n",
    "OUTPUT_DIR = Path(\"exp_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "NUM_TOTAL = 100   # total images in subset\n",
    "NUM_FROM_EASY = 50\n",
    "NUM_FROM_HARD = 50\n",
    "\n",
    "IMG_SIZE = (200, 64)   # from generator\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82fa3a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy items: 500, Hard items: 1000\n",
      "Unique labels in subset: 60\n",
      "{'acnestis', 'noise', 'python', 'ephemeral', 'dataset', 'neural', 'vision', 'model', 'learning', 'torch', 'captcha', 'byzantine'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# UTIL: read labels.json + list images\n",
    "# -------------------------\n",
    "def read_set(set_name):\n",
    "    p = ROOT / set_name\n",
    "    labels = json.load(open(p / \"labels.json\"))\n",
    "    images_dir = p / \"images\"\n",
    "    items = []\n",
    "    for fname, label in labels.items():\n",
    "        path = images_dir / fname\n",
    "        if path.exists():\n",
    "            items.append((str(path), label))\n",
    "    return items\n",
    "\n",
    "easy_items = read_set(\"easy\")\n",
    "hard_items = read_set(\"hard\")\n",
    "print(f\"Easy items: {len(easy_items)}, Hard items: {len(hard_items)}\")\n",
    "assert len(easy_items) >= NUM_FROM_EASY and len(hard_items) >= NUM_FROM_HARD, \\\n",
    "    f\"Not enough images: easy {len(easy_items)}, hard {len(hard_items)}\"\n",
    "\n",
    "# -------------------------\n",
    "# Select subset (random)\n",
    "# -------------------------\n",
    "selected = random.sample(easy_items, NUM_FROM_EASY) + random.sample(hard_items, NUM_FROM_HARD)\n",
    "random.shuffle(selected)\n",
    "\n",
    "# Build label -> index map for only labels present in selected\n",
    "labels_present = sorted(list({lbl for (_, lbl) in selected}))\n",
    "label2idx = {lbl:i for i,lbl in enumerate(labels_present)}\n",
    "idx2label = {i:l for l,i in label2idx.items()}\n",
    "print(f\"Unique labels in subset: {len(labels_present)}\")\n",
    "st = set()\n",
    "for labels in labels_present:\n",
    "    st.add(labels.lower())\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0947705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train 70, val 15, test 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Torch dataset\n",
    "# -------------------------\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, items, label2idx, transform=None):\n",
    "        self.items = items\n",
    "        self.label2idx = label2idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.items[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.label2idx[label]\n",
    "\n",
    "# transforms (augmentations)\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((64, 200)),   # (H,W)\n",
    "    transforms.RandomRotation(5, fill=(255,255,255)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05,0.1), shear=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((64, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = OCRDataset(selected, label2idx, transform=train_tf)\n",
    "\n",
    "# split into train/val/test\n",
    "n = len(dataset)\n",
    "n_train = int(0.7*n)\n",
    "n_val = int(0.15*n)\n",
    "n_test = n - n_train - n_val\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "\n",
    "# For val/test use deterministic transforms\n",
    "val_ds.dataset.transform = val_tf\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Split sizes: train {len(train_ds)}, val {len(val_ds)}, test {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a9478d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Model: Transfer learning (ResNet18)\n",
    "# -------------------------\n",
    "def get_model(num_classes, use_pretrained=True):\n",
    "    model = models.resnet18(pretrained=use_pretrained)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = get_model(len(labels_present), use_pretrained=True).to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Training utilities\n",
    "# -------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    golds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            out = model(imgs)\n",
    "            _, p = out.max(1)\n",
    "            correct += (p==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            preds.extend(p.cpu().tolist())\n",
    "            golds.extend(labels.cpu().tolist())\n",
    "    acc = correct/total if total>0 else 0.0\n",
    "    return acc, preds, golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed77d01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss 3.9234 | val_acc 0.0000\n",
      "Epoch 2 | train_loss 1.8326 | val_acc 0.0000\n",
      "Epoch 3 | train_loss 1.0573 | val_acc 0.2667\n",
      "Saved best model\n",
      "Epoch 4 | train_loss 0.7067 | val_acc 0.4000\n",
      "Saved best model\n",
      "Epoch 5 | train_loss 0.3624 | val_acc 0.4000\n",
      "Epoch 6 | train_loss 0.2740 | val_acc 0.4000\n",
      "Epoch 7 | train_loss 0.1501 | val_acc 0.4667\n",
      "Saved best model\n",
      "Epoch 8 | train_loss 0.1276 | val_acc 0.4667\n",
      "Epoch 9 | train_loss 0.1231 | val_acc 0.4667\n",
      "Epoch 10 | train_loss 0.1341 | val_acc 0.4667\n",
      "Epoch 11 | train_loss 0.0712 | val_acc 0.4667\n",
      "Epoch 12 | train_loss 0.0615 | val_acc 0.4667\n",
      "Epoch 13 | train_loss 0.0620 | val_acc 0.4667\n",
      "Epoch 14 | train_loss 0.0529 | val_acc 0.4667\n",
      "Epoch 15 | train_loss 0.0576 | val_acc 0.4667\n",
      "Epoch 16 | train_loss 0.0823 | val_acc 0.4667\n",
      "Epoch 17 | train_loss 0.0660 | val_acc 0.4667\n",
      "Epoch 18 | train_loss 0.0471 | val_acc 0.4667\n",
      "Epoch 19 | train_loss 0.0387 | val_acc 0.4667\n",
      "Epoch 20 | train_loss 0.0438 | val_acc 0.4667\n",
      "Epoch 21 | train_loss 0.0414 | val_acc 0.4667\n",
      "Epoch 22 | train_loss 0.0363 | val_acc 0.4667\n",
      "Epoch 23 | train_loss 0.0385 | val_acc 0.4667\n",
      "Epoch 24 | train_loss 0.0369 | val_acc 0.4667\n",
      "Epoch 25 | train_loss 0.0389 | val_acc 0.4667\n",
      "Epoch 26 | train_loss 0.0410 | val_acc 0.4667\n",
      "Epoch 27 | train_loss 0.0350 | val_acc 0.4667\n",
      "Epoch 28 | train_loss 0.0372 | val_acc 0.4667\n",
      "Epoch 29 | train_loss 0.0286 | val_acc 0.4667\n",
      "Epoch 30 | train_loss 0.0446 | val_acc 0.4667\n",
      "Epoch 31 | train_loss 0.0405 | val_acc 0.4667\n",
      "Epoch 32 | train_loss 0.0475 | val_acc 0.4667\n",
      "Epoch 33 | train_loss 0.0340 | val_acc 0.4667\n",
      "Epoch 34 | train_loss 0.0346 | val_acc 0.4667\n",
      "Epoch 35 | train_loss 0.0482 | val_acc 0.4667\n",
      "Epoch 36 | train_loss 0.0593 | val_acc 0.4667\n",
      "Epoch 37 | train_loss 0.0329 | val_acc 0.4667\n",
      "Epoch 38 | train_loss 0.0344 | val_acc 0.4667\n",
      "Epoch 39 | train_loss 0.0329 | val_acc 0.4667\n",
      "Epoch 40 | train_loss 0.0319 | val_acc 0.4667\n",
      "TEST ACCURACY: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Train loop\n",
    "# -------------------------\n",
    "best_val = 0.0\n",
    "for epoch in range(1, 40+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    val_acc, _, _ = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch} | train_loss {train_loss:.4f} | val_acc {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / \"best_model.pth\")\n",
    "        print(\"Saved best model\")\n",
    "\n",
    "# -------------------------\n",
    "# Final evaluation on test set\n",
    "# -------------------------\n",
    "model.load_state_dict(torch.load(OUTPUT_DIR / \"best_model.pth\"))\n",
    "test_acc, preds, golds = evaluate(model, test_loader)\n",
    "print(\"TEST ACCURACY:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c28a51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([57, 27, 13, 23, 33, 0, 45, 19, 11, 7, 25, 8, 39, 27, 21],\n",
       " [30, 27, 13, 1, 33, 34, 37, 9, 11, 7, 25, 8, 50, 27, 51])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69bebf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ACCURACY: 1.0\n"
     ]
    }
   ],
   "source": [
    "train_acc, preds, golds = evaluate(model, train_loader)\n",
    "print(\"train ACCURACY:\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f469ff00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2,\n",
       "  15,\n",
       "  31,\n",
       "  27,\n",
       "  33,\n",
       "  10,\n",
       "  54,\n",
       "  47,\n",
       "  31,\n",
       "  33,\n",
       "  5,\n",
       "  27,\n",
       "  25,\n",
       "  5,\n",
       "  17,\n",
       "  5,\n",
       "  6,\n",
       "  26,\n",
       "  12,\n",
       "  24,\n",
       "  28,\n",
       "  11,\n",
       "  36,\n",
       "  39,\n",
       "  53,\n",
       "  43,\n",
       "  35,\n",
       "  21,\n",
       "  13,\n",
       "  48,\n",
       "  58,\n",
       "  23,\n",
       "  56,\n",
       "  49,\n",
       "  59,\n",
       "  22,\n",
       "  0,\n",
       "  11,\n",
       "  33,\n",
       "  46,\n",
       "  16,\n",
       "  2,\n",
       "  25,\n",
       "  27,\n",
       "  7,\n",
       "  27,\n",
       "  38,\n",
       "  45,\n",
       "  25,\n",
       "  7,\n",
       "  17,\n",
       "  24,\n",
       "  40,\n",
       "  17,\n",
       "  42,\n",
       "  33,\n",
       "  27,\n",
       "  8,\n",
       "  33,\n",
       "  19,\n",
       "  25,\n",
       "  44,\n",
       "  57,\n",
       "  8,\n",
       "  27,\n",
       "  2,\n",
       "  20,\n",
       "  21,\n",
       "  52,\n",
       "  31],\n",
       " [2,\n",
       "  15,\n",
       "  31,\n",
       "  27,\n",
       "  33,\n",
       "  10,\n",
       "  54,\n",
       "  47,\n",
       "  31,\n",
       "  33,\n",
       "  5,\n",
       "  27,\n",
       "  25,\n",
       "  5,\n",
       "  17,\n",
       "  5,\n",
       "  6,\n",
       "  26,\n",
       "  12,\n",
       "  24,\n",
       "  28,\n",
       "  11,\n",
       "  36,\n",
       "  39,\n",
       "  53,\n",
       "  43,\n",
       "  35,\n",
       "  21,\n",
       "  13,\n",
       "  48,\n",
       "  58,\n",
       "  23,\n",
       "  56,\n",
       "  49,\n",
       "  59,\n",
       "  22,\n",
       "  0,\n",
       "  11,\n",
       "  33,\n",
       "  46,\n",
       "  16,\n",
       "  2,\n",
       "  25,\n",
       "  27,\n",
       "  7,\n",
       "  27,\n",
       "  38,\n",
       "  45,\n",
       "  25,\n",
       "  7,\n",
       "  17,\n",
       "  24,\n",
       "  40,\n",
       "  17,\n",
       "  42,\n",
       "  33,\n",
       "  27,\n",
       "  8,\n",
       "  33,\n",
       "  19,\n",
       "  25,\n",
       "  44,\n",
       "  57,\n",
       "  8,\n",
       "  27,\n",
       "  2,\n",
       "  20,\n",
       "  21,\n",
       "  52,\n",
       "  31])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, golds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee16aa3",
   "metadata": {},
   "source": [
    "5) Challenges you will face (and how to overcome them)\n",
    "\n",
    "Too few unique labels / too few per-class samples\n",
    "\n",
    "Mitigation: collect more images, generate synthetic variations (warp, elastic transform, color jitter), use class balancing and oversampling.\n",
    "\n",
    "Strong domain shift between easy & hard\n",
    "\n",
    "Mitigation: ensure your training set contains balanced mix of easy/hard; use domain-specific augmentation; use domain-adversarial training if needed.\n",
    "\n",
    "Confusions from visually similar words (e.g., \"Neural\" vs \"Neurall\" if typos)\n",
    "\n",
    "Mitigation: add character-level modeling (CTC) or sequence models; for classification, use top-k evaluation.\n",
    "\n",
    "Font variety vs font corruption\n",
    "\n",
    "Mitigation: use multiple legitimate fonts and validate font files; augment with synthetic fonts.\n",
    "\n",
    "Small input size / aspect ratio\n",
    "\n",
    "Your images are wide and short (200×64). ResNet expects larger square images. I used Resize((64,200)) then let ResNet process it. Alternatives:\n",
    "\n",
    "Use a custom lightweight CNN specialized for the aspect ratio (conv layers that preserve width).\n",
    "\n",
    "Use adaptive pooling before the linear head.\n",
    "\n",
    "Overfitting due to small dataset\n",
    "\n",
    "Mitigation: dropout, weight decay, strong augmentation, pretrained models, early stopping.\n",
    "\n",
    "Class imbalance (some words may appear more)\n",
    "\n",
    "Mitigation: balanced sampling, weighted loss.\n",
    "\n",
    "Resource constraints (CPU-only)\n",
    "\n",
    "Mitigation: reduce batch size, fewer epochs, use smaller models (MobileNet, lightweight CNN).\n",
    "\n",
    "6) Concrete suggestions to improve performance beyond the baseline\n",
    "\n",
    "Increase vocabulary and samples per class — the most impactful change.\n",
    "\n",
    "Use synthetic augmentation targeted to CAPTCHA distortions (curves, elastic warp, occlusion).\n",
    "\n",
    "Switch to a CRNN if you need robust sequence recognition — classification into fixed vocabulary is brittle for many labels.\n",
    "\n",
    "Precompute / cache features if training many experiments (faster).\n",
    "\n",
    "Use cross-validation for robust estimates given small dataset.\n",
    "\n",
    "7) Deliverables I can provide right now\n",
    "\n",
    "The runnable PyTorch script above (done).\n",
    "\n",
    "A second script or notebook to run the full SPC experiments (1,5,10,25,all) and plot learning curves + produce CSV of results (I can supply that code too).\n",
    "\n",
    "A short report template (markdown) that you can fill with actual numbers after running the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec455ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Byzantine       1.00      1.00      1.00         1\n",
      "     DAtaSet       0.00      0.00      0.00         1\n",
      "     Dataset       1.00      1.00      1.00         2\n",
      "   Ephemeral       1.00      1.00      1.00         1\n",
      "    Learning       1.00      1.00      1.00         1\n",
      "       Model       0.50      1.00      0.67         1\n",
      "       Noise       0.00      0.00      0.00         1\n",
      "      Python       1.00      1.00      1.00         2\n",
      "       Torch       1.00      1.00      1.00         1\n",
      "      VisioN       0.00      0.00      0.00         1\n",
      "    acNEsTIs       0.00      0.00      0.00         1\n",
      "       torCh       0.00      0.00      0.00         1\n",
      "       torcH       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.90      0.60      0.72        15\n",
      "   macro avg       0.50      0.54      0.51        15\n",
      "weighted avg       0.57      0.60      0.58        15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'LeArnIng'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m subset_label2idx = {v:k \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m subset_idx2label.items()}\n\u001b[32m     15\u001b[39m golds_subset = [subset_label2idx[idx2label[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m golds]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m preds_subset = [\u001b[43msubset_label2idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[32m     18\u001b[39m cm = confusion_matrix(golds_subset, preds_subset)\n\u001b[32m     19\u001b[39m report = classification_report(golds_subset, preds_subset, target_names=[subset_idx2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(unique_classes))])\n",
      "\u001b[31mKeyError\u001b[39m: 'LeArnIng'"
     ]
    }
   ],
   "source": [
    "# Get unique class indices in current evaluation\n",
    "unique_classes = sorted(list(set(golds)))\n",
    "target_names = [idx2label[i] for i in unique_classes]\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(golds, preds, labels=unique_classes)\n",
    "report = classification_report(golds, preds, labels=unique_classes, target_names=target_names)\n",
    "print(report)\n",
    "\n",
    "# Map preds/golds to a continuous 0..N-1 range for subset only\n",
    "subset_idx2label = {i: idx2label[i] for i in unique_classes}\n",
    "subset_label2idx = {v:k for k,v in subset_idx2label.items()}\n",
    "\n",
    "golds_subset = [subset_label2idx[idx2label[i]] for i in golds]\n",
    "preds_subset = [subset_label2idx[idx2label[i]] for i in preds]\n",
    "\n",
    "cm = confusion_matrix(golds_subset, preds_subset)\n",
    "report = classification_report(golds_subset, preds_subset, target_names=[subset_idx2label[i] for i in range(len(unique_classes))])\n",
    "\n",
    "\n",
    "# Save artifacts\n",
    "with open(OUTPUT_DIR / \"label_map.json\", \"w\") as f:\n",
    "    json.dump(label2idx, f, indent=2)\n",
    "\n",
    "# Save basic stats\n",
    "with open(OUTPUT_DIR / \"stats.txt\", \"w\") as f:\n",
    "    f.write(f\"labels_count: {len(labels_present)}\\n\")\n",
    "    f.write(f\"split: train {len(train_ds)} val {len(val_ds)} test {len(test_ds)}\\n\")\n",
    "    f.write(f\"best_val: {best_val}\\n\")\n",
    "    f.write(f\"test_acc: {test_acc}\\n\")\n",
    "\n",
    "print(\"Done. Artifacts in\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ba9ce",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efa1cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- Block 1 --------\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # /2\n",
    "        )\n",
    "\n",
    "        # -------- Block 2 --------\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # /4\n",
    "        )\n",
    "\n",
    "        # -------- Block 3 --------\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # /8\n",
    "        )\n",
    "\n",
    "        # -------- Block 4 --------\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # /16\n",
    "        )\n",
    "\n",
    "        # Adaptive pooling → fixed size\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e179205f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=60, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomCNN(num_classes=len(labels_present)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,           # start higher, reduce if unstable\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=10,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4e83df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss 4.3949 | val_acc 0.0667\n",
      "Saved best model\n",
      "Epoch 2 | train_loss 3.8416 | val_acc 0.0000\n",
      "Epoch 3 | train_loss 3.5484 | val_acc 0.0000\n",
      "Epoch 4 | train_loss 3.6879 | val_acc 0.0667\n",
      "Epoch 5 | train_loss 3.3494 | val_acc 0.0667\n",
      "Epoch 6 | train_loss 3.2717 | val_acc 0.0667\n",
      "Epoch 7 | train_loss 3.1430 | val_acc 0.1333\n",
      "Saved best model\n",
      "Epoch 8 | train_loss 3.0096 | val_acc 0.2000\n",
      "Saved best model\n",
      "Epoch 9 | train_loss 3.0198 | val_acc 0.1333\n",
      "Epoch 10 | train_loss 2.8064 | val_acc 0.0000\n",
      "Epoch 11 | train_loss 2.9345 | val_acc 0.0000\n",
      "Epoch 12 | train_loss 2.6339 | val_acc 0.1333\n",
      "Epoch 13 | train_loss 2.5681 | val_acc 0.1333\n",
      "Epoch 14 | train_loss 2.5262 | val_acc 0.2000\n",
      "Epoch 15 | train_loss 2.3639 | val_acc 0.2000\n",
      "Epoch 16 | train_loss 2.3423 | val_acc 0.2000\n",
      "Epoch 17 | train_loss 2.2730 | val_acc 0.0667\n",
      "Epoch 18 | train_loss 2.2692 | val_acc 0.2667\n",
      "Saved best model\n",
      "Epoch 19 | train_loss 2.2770 | val_acc 0.2667\n",
      "Epoch 20 | train_loss 2.0375 | val_acc 0.2000\n",
      "Epoch 21 | train_loss 2.1213 | val_acc 0.2000\n",
      "Epoch 22 | train_loss 1.9328 | val_acc 0.2667\n",
      "Epoch 23 | train_loss 2.0950 | val_acc 0.2667\n",
      "Epoch 24 | train_loss 1.8692 | val_acc 0.3333\n",
      "Saved best model\n",
      "Epoch 25 | train_loss 1.7851 | val_acc 0.2667\n",
      "Epoch 26 | train_loss 1.8850 | val_acc 0.3333\n",
      "Epoch 27 | train_loss 1.8343 | val_acc 0.3333\n",
      "Epoch 28 | train_loss 1.8187 | val_acc 0.2667\n",
      "Epoch 29 | train_loss 1.7531 | val_acc 0.3333\n",
      "Epoch 30 | train_loss 1.6759 | val_acc 0.2667\n",
      "Epoch 31 | train_loss 1.6240 | val_acc 0.3333\n",
      "Epoch 32 | train_loss 1.5845 | val_acc 0.4667\n",
      "Saved best model\n",
      "Epoch 33 | train_loss 1.4911 | val_acc 0.4000\n",
      "Epoch 34 | train_loss 1.5565 | val_acc 0.4667\n",
      "Epoch 35 | train_loss 1.5218 | val_acc 0.4667\n",
      "Epoch 36 | train_loss 1.4714 | val_acc 0.4000\n",
      "Epoch 37 | train_loss 1.4992 | val_acc 0.4667\n",
      "Epoch 38 | train_loss 1.3995 | val_acc 0.4667\n",
      "Epoch 39 | train_loss 1.5721 | val_acc 0.4667\n",
      "Epoch 40 | train_loss 1.4888 | val_acc 0.4667\n",
      "TEST ACCURACY: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "best_val = 0.0\n",
    "for epoch in range(1, 40+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    val_acc, _, _ = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch} | train_loss {train_loss:.4f} | val_acc {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / \"best_model.pth\")\n",
    "        print(\"Saved best model\")\n",
    "\n",
    "# -------------------------\n",
    "# Final evaluation on test set\n",
    "# -------------------------\n",
    "model.load_state_dict(torch.load(OUTPUT_DIR / \"best_model.pth\"))\n",
    "test_acc, preds, golds = evaluate(model, test_loader)\n",
    "print(\"TEST ACCURACY:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b4950bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ACCURACY: 0.6714285714285714\n"
     ]
    }
   ],
   "source": [
    "train_acc, preds, golds = evaluate(model, train_loader)\n",
    "print(\"train ACCURACY:\", train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441ea3c",
   "metadata": {},
   "source": [
    "✅ Residual CNN (from scratch)\n",
    "\n",
    "✅ SE (Squeeze-and-Excitation) Attention\n",
    "\n",
    "✅ CBAM Attention (Channel + Spatial)\n",
    "\n",
    "✅ CNN + BiLSTM for OCR-style tasks\n",
    "\n",
    "✅ Confusion Matrix + Per-Class Accuracy\n",
    "\n",
    "✅ Mixed Precision Training (AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03f925ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Residual Block:\n",
    "    Conv → BN → ReLU → Conv → BN + Skip Connection\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels,\n",
    "            kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Projection if dimensions change\n",
    "        self.shortcut = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        out += identity\n",
    "        return F.relu(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7bd676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation block\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "\n",
    "        # Channel Attention\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels)\n",
    "        )\n",
    "\n",
    "        # Spatial Attention\n",
    "        self.spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        # Channel attention\n",
    "        avg = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
    "        mx = F.adaptive_max_pool2d(x, 1).view(b, c)\n",
    "        channel_attn = torch.sigmoid(self.mlp(avg) + self.mlp(mx)).view(b, c, 1, 1)\n",
    "        x = x * channel_attn\n",
    "\n",
    "        # Spatial attention\n",
    "        avg_map = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_map, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_attn = torch.sigmoid(self.spatial(torch.cat([avg_map, max_map], dim=1)))\n",
    "\n",
    "        return x * spatial_attn\n",
    "\n",
    "\n",
    "class AdvancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes, use_cbam=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = ResidualBlock(64, 128, stride=2)\n",
    "        self.layer2 = ResidualBlock(128, 256, stride=2)\n",
    "        self.layer3 = ResidualBlock(256, 512, stride=2)\n",
    "\n",
    "        self.attn = CBAM(512) if use_cbam else SEBlock(512)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.attn(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)        # (B, C, H, W)\n",
    "        x = x.mean(dim=2)      # collapse height → (B, C, W)\n",
    "        x = x.permute(0, 2, 1) # (B, W, C)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25f0f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kn/hj6mf2_91pjg37k1lp8x8j7c0000gn/T/ipykernel_35602/3676872944.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/var/folders/kn/hj6mf2_91pjg37k1lp8x8j7c0000gn/T/ipykernel_35602/3676872944.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | loss 4.0897 | val_acc 0.0000\n",
      "Epoch 2 | loss 4.0900 | val_acc 0.0000\n",
      "Epoch 3 | loss 4.0831 | val_acc 0.0000\n",
      "Epoch 4 | loss 4.0888 | val_acc 0.0000\n",
      "Epoch 5 | loss 4.0825 | val_acc 0.0000\n",
      "Epoch 6 | loss 4.0911 | val_acc 0.0000\n",
      "Epoch 7 | loss 4.0864 | val_acc 0.0000\n",
      "Epoch 8 | loss 4.0973 | val_acc 0.0000\n",
      "Epoch 9 | loss 4.0918 | val_acc 0.0000\n",
      "Epoch 10 | loss 4.0916 | val_acc 0.0000\n",
      "Epoch 11 | loss 4.0889 | val_acc 0.0000\n",
      "Epoch 12 | loss 4.0940 | val_acc 0.0000\n",
      "Epoch 13 | loss 4.0831 | val_acc 0.0000\n",
      "Epoch 14 | loss 4.0903 | val_acc 0.0000\n",
      "Epoch 15 | loss 4.0921 | val_acc 0.0000\n",
      "Epoch 16 | loss 4.0868 | val_acc 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m optimizer.zero_grad()\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m     31\u001b[39m scaler.scale(loss).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mAdvancedCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     78\u001b[39m x = \u001b[38;5;28mself\u001b[39m.stem(x)\n\u001b[32m     79\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer3(x)\n\u001b[32m     83\u001b[39m x = \u001b[38;5;28mself\u001b[39m.attn(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mResidualBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     30\u001b[39m identity = \u001b[38;5;28mself\u001b[39m.shortcut(x)\n\u001b[32m     32\u001b[39m out = F.relu(\u001b[38;5;28mself\u001b[39m.bn1(\u001b[38;5;28mself\u001b[39m.conv1(x)))\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     35\u001b[39m out += identity\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.relu(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ocr/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(preds, labels, class_names):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for i, acc in enumerate(per_class_acc):\n",
    "        print(f\"{class_names[i]}: {acc:.4f}\")\n",
    "\n",
    "    return cm\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "model = AdvancedCNN(num_classes=len(labels_present), use_cbam=True).to(DEVICE)\n",
    "for epoch in range(1, 20 + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    val_acc, _, _ = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch} | loss {running_loss/len(train_loader.dataset):.4f} | val_acc {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e964d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
